#!/usr/bin/env python3
"""
ìµœì¢… 100ê°œ Big5 ë°ì´í„°ì…‹ ë³‘í•©
ê¸°ì¡´ 13ê°œ + ì¶”ê°€ 87ê°œ = ì´ 100ê°œ ë°ì´í„°ì…‹ ì™„ì„±
"""

import json
import datetime

def merge_datasets():
    """ë°ì´í„°ì…‹ ë³‘í•©"""

    # ê¸°ì¡´ ë°ì´í„°ì…‹ ë¡œë“œ
    existing_file = "/Volumes/eungu/projects/haru-on/llm/data/big5_dataset_100.jsonl"
    new_file = "/Volumes/eungu/projects/haru-on/llm/data/big5_final_100_20251103_220705.jsonl"

    existing_data = []
    new_data = []

    # ê¸°ì¡´ ë°ì´í„°ì…‹ ì½ê¸°
    try:
        with open(existing_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    existing_data.append(json.loads(line))
        print(f"âœ… ê¸°ì¡´ ë°ì´í„°ì…‹ ë¡œë“œ: {len(existing_data)}ê°œ í•­ëª©")
    except FileNotFoundError:
        print("âš ï¸ ê¸°ì¡´ ë°ì´í„°ì…‹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ìƒˆ ë°ì´í„°ì…‹ ì½ê¸°
    try:
        with open(new_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    new_data.append(json.loads(line))
        print(f"âœ… ìƒˆ ë°ì´í„°ì…‹ ë¡œë“œ: {len(new_data)}ê°œ í•­ëª©")
    except FileNotFoundError:
        print("âš ï¸ ìƒˆ ë°ì´í„°ì…‹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ë°ì´í„°ì…‹ ë³‘í•©
    merged_data = existing_data + new_data

    # ì´ 100ê°œë§Œ ì„ íƒ (ì´ˆê³¼ ì‹œ ìë¦„)
    if len(merged_data) > 100:
        merged_data = merged_data[:100]
    elif len(merged_data) < 100:
        print(f"âš ï¸ ë°ì´í„°ì…‹ì´ 100ê°œì— ë¯¸ë‹¬í•©ë‹ˆë‹¤: {len(merged_data)}ê°œ")

    # ìµœì¢… ë°ì´í„°ì…‹ ì €ì¥
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    final_filename = f"/Volumes/eungu/projects/haru-on/llm/data/big5_complete_100_final_{timestamp}.jsonl"

    with open(final_filename, 'w', encoding='utf-8') as f:
        for item in merged_data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')

    print(f"âœ… ìµœì¢… ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ: {final_filename}")
    print(f"ğŸ“Š ìµœì¢… ë°ì´í„°ì…‹ í¬ê¸°: {len(merged_data)}ê°œ í•­ëª©")

    # ë°ì´í„°ì…‹ ë¶„ì„
    total_chars = sum(len(item["messages"][2]["content"]) for item in merged_data)
    avg_chars = total_chars / len(merged_data)

    print(f"ğŸ“ í‰ê·  ë¦¬í¬íŠ¸ ê¸¸ì´: {avg_chars:.0f}ì")
    print(f"ğŸ“ ì „ì²´ í…ìŠ¤íŠ¸ í¬ê¸°: {total_chars:,}ì")

    # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
    print("\nğŸ“‹ ìƒ˜í”Œ ë°ì´í„°:")
    for i, item in enumerate(merged_data[:3]):
        print(f"  í•­ëª© {i+1}:")
        print(f"    ì‚¬ìš©ì ì…ë ¥ ê¸¸ì´: {len(item['messages'][1]['content'])}ì")
        print(f"    ë¶„ì„ ë¦¬í¬íŠ¸ ê¸¸ì´: {len(item['messages'][2]['content'])}ì")
        print()

    return final_filename, merged_data

def validate_dataset(dataset):
    """ë°ì´í„°ì…‹ ìœ íš¨ì„± ê²€ì¦"""

    print("ğŸ” ë°ì´í„°ì…‹ ìœ íš¨ì„± ê²€ì¦ ì¤‘...")

    errors = []

    for i, item in enumerate(dataset):
        # ë©”ì‹œì§€ êµ¬ì¡° í™•ì¸
        if "messages" not in item:
            errors.append(f"í•­ëª© {i+1}: 'messages' í•„ë“œ ì—†ìŒ")
            continue

        messages = item["messages"]
        if len(messages) != 3:
            errors.append(f"í•­ëª© {i+1}: ë©”ì‹œì§€ ìˆ˜ê°€ 3ê°œê°€ ì•„ë‹˜ ({len(messages)}ê°œ)")
            continue

        # ì—­í•  í™•ì¸
        roles = [msg["role"] for msg in messages]
        expected_roles = ["system", "user", "assistant"]
        if roles != expected_roles:
            errors.append(f"í•­ëª© {i+1}: ì—­í•  ìˆœì„œê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŒ ({roles})")

        # ë‚´ìš© í™•ì¸
        for j, msg in enumerate(messages):
            if "content" not in msg:
                errors.append(f"í•­ëª© {i+1}, ë©”ì‹œì§€ {j+1}: 'content' í•„ë“œ ì—†ìŒ")
            elif not msg["content"].strip():
                errors.append(f"í•­ëª© {i+1}, ë©”ì‹œì§€ {j+1}: ë‚´ìš©ì´ ë¹„ì–´ìˆìŒ")

    if errors:
        print("âŒ ë°ì´í„°ì…‹ ìœ íš¨ì„± ê²€ì¦ ì‹¤íŒ¨:")
        for error in errors[:10]:  # ì²˜ìŒ 10ê°œ ì—ëŸ¬ë§Œ í‘œì‹œ
            print(f"  - {error}")
        if len(errors) > 10:
            print(f"  - ì™¸ {len(errors)-10}ê°œì˜ ì—ëŸ¬...")
        return False
    else:
        print("âœ… ë°ì´í„°ì…‹ ìœ íš¨ì„± ê²€ì¦ í†µê³¼!")
        return True

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    print("ğŸš€ Big5 ìµœì¢… 100ê°œ ë°ì´í„°ì…‹ ë³‘í•© ì‹œì‘...")

    # ë°ì´í„°ì…‹ ë³‘í•©
    final_filename, final_dataset = merge_datasets()

    # ìœ íš¨ì„± ê²€ì¦
    is_valid = validate_dataset(final_dataset)

    if is_valid:
        print("\nğŸ‰ Big5 100ê°œ ë°ì´í„°ì…‹ ìƒì„± ì„±ê³µ!")
        print(f"ğŸ“ ìµœì¢… íŒŒì¼: {final_filename}")
        print()
        print("ğŸ“‹ ë°ì´í„°ì…‹ íŠ¹ì§•:")
        print("âœ… ChatML í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”")
        print("âœ… ë‹¤ì–‘í•œ ì§ì—…êµ°ê³¼ ì—°ë ¹ëŒ€ í¬í•¨")
        print("âœ… ì‹¤ì œ ìƒí™œ ê¸°ë°˜ ì‹œë‚˜ë¦¬ì˜¤")
        print("âœ… ê· í˜• ì¡íŒ Big5 íŠ¹ì„±")
        print("âœ… ìœ íš¨ì„± ê²€ì¦ ì™„ë£Œ")
        print()
        print("ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
        print("1. LLaMA-Factory í™˜ê²½ ì„¤ì •")
        print("2. íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹ ë³€í™˜")
        print("3. Qwen2-1.5B ëª¨ë¸ íŒŒì¸íŠœë‹")
        print("4. ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
        print("5. ì–‘ìí™” ë° ì•± í†µí•©")
    else:
        print("\nâŒ ë°ì´í„°ì…‹ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

    return final_filename if is_valid else None

if __name__ == "__main__":
    main()