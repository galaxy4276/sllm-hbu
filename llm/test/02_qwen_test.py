#!/usr/bin/env python3
"""
Qwen2-1.5B ëª¨ë¸ ê¸°ë³¸ í…ŒìŠ¤íŠ¸
ëª¨ë¸ ë¡œë”© ë° ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
"""

import torch
import time
from transformers import AutoTokenizer, AutoModelForCausalLM

def test_qwen_model():
    """Qwen2-1.5B ëª¨ë¸ í…ŒìŠ¤íŠ¸"""

    print("ğŸ“¦ Qwen2-1.5B ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    # ì¥ì¹˜ ì„¤ì •
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    print(f"ğŸ”¥ ì‚¬ìš© ì¥ì¹˜: {device}")

    try:
        # ëª¨ë¸ ì •ë³´
        model_name = "Qwen/Qwen2-1.5B"
        print(f"ğŸ“‹ ëª¨ë¸: {model_name}")

        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        print("\nğŸ”¤ í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
        start_time = time.time()
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        tokenizer_load_time = time.time() - start_time
        print(f"âœ… í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ: {tokenizer_load_time:.2f}ì´ˆ")

        # ëª¨ë¸ ë¡œë“œ
        print("\nğŸ¤– ëª¨ë¸ ë¡œë”© ì¤‘...")
        start_time = time.time()

        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )

        model_load_time = time.time() - start_time
        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_load_time:.2f}ì´ˆ")

        # ëª¨ë¸ ì •ë³´
        total_params = sum(p.numel() for p in model.parameters())
        print(f"ğŸ“Š ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}")

        return model, tokenizer, device

    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return None, None, device

def test_basic_inference(model, tokenizer, device):
    """ê¸°ë³¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸"""

    print("\nğŸ§ª ê¸°ë³¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸")
    print("-" * 30)

    test_prompts = [
        "ì•ˆë…•í•˜ì„¸ìš”",
        "ë‹¹ì‹ ì€ ì‹¬ë¦¬ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
        "Big5 ì„±ê²© íŠ¹ì„±ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."
    ]

    for i, prompt in enumerate(test_prompts, 1):
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ {i}: {prompt[:20]}...")

        try:
            # ì…ë ¥ í† í°í™”
            inputs = tokenizer(prompt, return_tensors="pt").to(device)

            # ì¶”ë¡  ì‹¤í–‰
            start_time = time.time()

            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=50,
                    do_sample=True,
                    temperature=0.7,
                    pad_token_id=tokenizer.eos_token_id
                )

            if device.type == "mps":
                torch.mps.synchronize()

            inference_time = time.time() - start_time

            # ê²°ê³¼ ë””ì½”ë”©
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)

            # ìƒì„±ëœ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
            generated_text = response[len(prompt):].strip()

            print(f"â±ï¸  ì¶”ë¡  ì‹œê°„: {inference_time:.2f}ì´ˆ")
            print(f"ğŸ“„ ìƒì„± ê²°ê³¼: {generated_text[:100]}...")

        except Exception as e:
            print(f"âŒ ì¶”ë¡  ì‹¤íŒ¨: {e}")

def test_big5_scenario(model, tokenizer, device):
    """Big5 ì‹¬ë¦¬ ë¶„ì„ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""

    print("\nğŸ¯ Big5 ì‹¬ë¦¬ ë¶„ì„ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸")
    print("-" * 40)

    # í…ŒìŠ¤íŠ¸ìš© Big5 ì§ˆë¬¸
    big5_prompt = """ë‹¹ì‹ ì€ Big5 ì‹¬ë¦¬ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì‚¬ìš©ì ë‹µë³€ì— ëŒ€í•´ ê°„ë‹¨íˆ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ë‹µë³€:
1. ê°œë°©ì„±: "ìƒˆë¡œìš´ ê²ƒì„ ë°°ìš°ëŠ” ê±¸ ì¢‹ì•„í•´ìš”"
2. ì„±ì‹¤ì„±: "ê³„íšì„ ì„¸ìš°ê³  ì§€í‚¤ëŠ” ê²Œ ì¤‘ìš”í•´ìš”"

ë¶„ì„:"""

    print(f"ğŸ“ í”„ë¡¬í”„íŠ¸: Big5 ì‹¬ë¦¬ ë¶„ì„ ì‹œë‚˜ë¦¬ì˜¤")
    print(f"ğŸ“ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(big5_prompt)}ì")

    try:
        # ì…ë ¥ í† í°í™”
        inputs = tokenizer(big5_prompt, return_tensors="pt").to(device)

        # ì…ë ¥ í† í° ìˆ˜ í™•ì¸
        input_tokens = inputs["input_ids"].size(1)
        print(f"ğŸ”¤ ì…ë ¥ í† í° ìˆ˜: {input_tokens}")

        # ì¶”ë¡  ì‹¤í–‰
        start_time = time.time()

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=150,  # ë” ê¸´ ì‘ë‹µ
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.eos_token_id
            )

        if device.type == "mps":
            torch.mps.synchronize()

        inference_time = time.time() - start_time

        # ê²°ê³¼ ë””ì½”ë”©
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # ìƒì„±ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        generated_text = response[len(big5_prompt):].strip()
        generated_tokens = len(tokenizer.encode(generated_text))

        print(f"â±ï¸  ì¶”ë¡  ì‹œê°„: {inference_time:.2f}ì´ˆ")
        print(f"ğŸ”¤ ìƒì„± í† í° ìˆ˜: {generated_tokens}")
        print(f"âš¡ í† í°/ì´ˆ: {generated_tokens/inference_time:.1f}")

        print(f"\nğŸ“„ ìƒì„±ëœ ë¦¬í¬íŠ¸:")
        print("-" * 20)
        print(generated_text)
        print("-" * 20)

        # ê°„ë‹¨í•œ í˜ë¥´ì†Œë‚˜ ê²€ì¦
        forbidden_patterns = ["ì§„ë‹¨", "ì…ë‹ˆë‹¤", "í™•ì‹¤í•©ë‹ˆë‹¤", "ë¶„ëª…í•©ë‹ˆë‹¤"]
        persona_score = 1.0

        for pattern in forbidden_patterns:
            if pattern in generated_text:
                persona_score -= 0.2
                print(f"âš ï¸  ê¸ˆì§€ í‘œí˜„ ë°œê²¬: '{pattern}'")

        print(f"ğŸ­ í˜ë¥´ì†Œë‚˜ ì ìˆ˜: {persona_score:.1f}/1.0")

        return True

    except Exception as e:
        print(f"âŒ Big5 ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_memory_usage():
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸"""

    print("\nğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰")
    print("-" * 20)

    try:
        import psutil
        import os

        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()

        print(f"ğŸ–¥ï¸  í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬: {memory_info.rss / (1024**2):.1f} MB")
        print(f"ğŸ’¾ ê°€ìƒ ë©”ëª¨ë¦¬: {memory_info.vms / (1024**2):.1f} MB")

        # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬
        memory = psutil.virtual_memory()
        print(f"ğŸ“Š ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {memory.percent:.1f}%")

        return True

    except ImportError:
        print("âš ï¸  psutilì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""

    print("ğŸš€ Qwen2-1.5B ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    print()

    # ëª¨ë¸ í…ŒìŠ¤íŠ¸
    model, tokenizer, device = test_qwen_model()

    if model is None or tokenizer is None:
        print("\nâŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨. í…ŒìŠ¤íŠ¸ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        return

    # ê¸°ë³¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸
    test_basic_inference(model, tokenizer, device)

    # Big5 ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
    big5_success = test_big5_scenario(model, tokenizer, device)

    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    memory_success = test_memory_usage()

    # ê²°ê³¼ ìš”ì•½
    print("\nğŸ¯ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 30)
    print(f"ëª¨ë¸ ë¡œë”©: âœ…")
    print(f"ê¸°ë³¸ ì¶”ë¡ : âœ…")
    print(f"Big5 ì‹œë‚˜ë¦¬ì˜¤: {'âœ…' if big5_success else 'âŒ'}")
    print(f"ë©”ëª¨ë¦¬ ì •ë³´: {'âœ…' if memory_success else 'âŒ'}")

    if big5_success:
        print("\nğŸ‰ Qwen2-1.5B ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        print("ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„: python 03_dataset_creation.py")
        print("ğŸ’¡ ì´ì œ Big5 ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ê³  íŒŒì¸íŠœë‹ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâš ï¸  ëª¨ë¸ í…ŒìŠ¤íŠ¸ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")

    print("\n" + "=" * 50)

if __name__ == "__main__":
    main()