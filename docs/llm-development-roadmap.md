# ì˜¨ë””ë°”ì´ìŠ¤ LLM ê°œë°œ ì™„ì „ ê°€ì´ë“œ: Qwen2-1.5B íŒŒì¸íŠœë‹ë¶€í„° ì•± ë°°í¬ê¹Œì§€

## ğŸ¯ ê°œìš”

ë³¸ ë¬¸ì„œëŠ” Big5 ì‹¬ë¦¬ ë¶„ì„ ì•± 'í•˜ë£¨ì˜¨'ì„ ìœ„í•œ ì˜¨ë””ë°”ì´ìŠ¤ LLM ê°œë°œ ì „ ê³¼ì •ì„ ìƒì„¸íˆ ë‹¤ë£¹ë‹ˆë‹¤. Qwen2-1.5B ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ì—¬ ì‹¬ë¦¬ ë¶„ì„ ì „ë¬¸ê°€ë¡œ íŠ¹í™”í•˜ê³ , ëª¨ë°”ì¼ ê¸°ê¸°ì—ì„œ êµ¬ë™ ê°€ëŠ¥í•˜ë„ë¡ ì–‘ìí™”í•˜ëŠ” ì „ì²´ ê³¼ì •ì„ í¬í•¨í•©ë‹ˆë‹¤.

### ğŸ“‹ ìµœì¢… ëª©í‘œ
```
ì›ë³¸ Qwen2-1.5B â†’ Big5 ì „ë¬¸ê°€ íŒŒì¸íŠœë‹ â†’ GGUF ë³€í™˜ â†’ 4ë¹„íŠ¸ ì–‘ìí™” â†’ iOS ì•± íƒ‘ì¬
```

### ğŸš€ ì˜ˆìƒ ì†Œìš” ì‹œê°„: 2-3ì£¼ (ì´ˆê¸‰ì ê¸°ì¤€)

---

## ğŸ“š ì‚¬ì „ ì¤€ë¹„ ì‚¬í•­

### í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­
- **GPU**: VRAM 16GB+ (RTX 4090, A100, H100 ê¶Œì¥)
- **RAM**: 32GB ì´ìƒ
- **ì €ì¥ ê³µê°„**: 100GB ì´ìƒ ì—¬ìœ  ê³µê°„
- **OS**: Linux (Ubuntu 20.04+) ë˜ëŠ” macOS (Apple Silicon ê¶Œì¥)

### ì†Œí”„íŠ¸ì›¨ì–´ ìš”êµ¬ì‚¬í•­
- Python 3.9+
- CUDA 11.8+ (NVIDIA GPU ì‚¬ìš© ì‹œ)
- Git
- Conda (ê¶Œì¥)

---

## ğŸ—ºï¸ 7ë‹¨ê³„ ê°œë°œ ë¡œë“œë§µ

### 1ë‹¨ê³„: ê°œë°œ í™˜ê²½ ì„¤ì • (1-2ì¼)

#### 1.1 í•˜ë“œì›¨ì–´ í™˜ê²½ í™•ì¸
```bash
# GPU í™•ì¸
nvidia-smi
# ì¶œë ¥ ì˜ˆì‹œ: Tesla V100-SXM2-32GB, 32510MiB

# CPU ë° RAM í™•ì¸
lscpu
free -h

# ì €ì¥ ê³µê°„ í™•ì¸
df -h
```

#### 1.2 Conda í™˜ê²½ ì„¤ì •
```bash
# ì „ìš© í™˜ê²½ ìƒì„±
conda create -n qwen-tuning python=3.10 -y
conda activate qwen-tuning

# PyTorch ì„¤ì¹˜ (CUDA ë²„ì „ í™•ì¸ í›„)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# GPU í…ŒìŠ¤íŠ¸
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
```

#### 1.3 í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```bash
# Hugging Face ìƒíƒœê³„
pip install transformers datasets accelerate bitsandbytes peft

# íŠœë‹ í”„ë ˆì„ì›Œí¬
pip install trl wandb tensorboard

# ìœ í‹¸ë¦¬í‹°
pip install tqdm numpy pandas matplotlib

# GGUF ë³€í™˜ìš© (llama.cpp)
pip install sentencepiece protobuf
```

---

### 2ë‹¨ê³„: ë°ì´í„°ì…‹ êµ¬ì¶• (3-5ì¼) â­ **ê°€ì¥ ì¤‘ìš”í•œ ë‹¨ê³„**

#### 2.1 ë°ì´í„° êµ¬ì¡° ì„¤ê³„

**ChatML í˜•ì‹ (Qwen2 ê¶Œì¥)**:
```json
{
  "messages": [
    {
      "role": "system",
      "content": "ë‹¹ì‹ ì€ Big5 ì‹¬ë¦¬í•™ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ë‹µë³€ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ ì‹¬ë¦¬ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ 5ê°€ì§€ ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ, ê° íŠ¹ì„±(ê°œë°©ì„±, ì„±ì‹¤ì„±, ì™¸í–¥ì„±, ìš°í˜¸ì„±, ì‹ ê²½ì„±)ì„ ë¶„ì„í•˜ê³  ê¸ì •ì ì´ë©° í†µì°°ë ¥ ìˆëŠ” ì¢…í•© ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ì ˆëŒ€ ì˜í•™ì  ì§„ë‹¨ì„ ë‚´ë¦¬ì§€ ë§ˆì„¸ìš”."
    },
    {
      "role": "user",
      "content": "1. ê°œë°©ì„±: 'ìš”ì¦˜ì€ ì£¼ë§ë§ˆë‹¤ ìƒˆë¡œìš´ ë² ì´í‚¹ ë ˆì‹œí”¼ë¥¼ ë°°ìš°ëŠ” ê²Œ ê°€ì¥ ì¦ê±°ì›Œìš”. ìƒˆë¡œìš´ ê±¸ ë°°ìš°ëŠ” ê²ƒ ìì²´ê°€ ì—ë„ˆì§€ë¥¼ ì¤ë‹ˆë‹¤.'\n2. ì„±ì‹¤ì„±: 'ê³„íší–ˆë˜ ì¼ì„ ë§ˆì¹˜ë©´ ì •ë§ ë¿Œë“¯í•˜ê³ , ìŠ¤ìŠ¤ë¡œì— ëŒ€í•œ ìì‹ ê°ì´ ìƒê²¨ìš”. ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ ì§€ì›Œë‚˜ê°€ëŠ” ì¾Œê°ì´ ìˆìŠµë‹ˆë‹¤.'\n3. ì™¸í–¥ì„±: 'ì†”ì§íˆ í˜¼ì ìˆì„ ë•Œê°€ ë” í¸í•´ìš”. ì‚¬ëŒë“¤ê³¼ ìˆëŠ” ê²ƒë„ ì¢‹ì§€ë§Œ, ê²°êµ­ ì—ë„ˆì§€ëŠ” í˜¼ì ì±… ì½ê±°ë‚˜ ì‰¬ë©´ì„œ ì¶©ì „ë¼ìš”.'\n4. ìš°í˜¸ì„±: 'ì¹œêµ¬ê°€ í˜ë“¤ì–´í•˜ë©´ ì¼ë‹¨ ë¬´ìŠ¨ ì¼ì¸ì§€ ëê¹Œì§€ ë“¤ì–´ì£¼ë ¤ê³  ë…¸ë ¥í•´ìš”. ê³µê°í•´ì£¼ê³  ì˜†ì— ìˆì–´ ì£¼ëŠ” í¸ì…ë‹ˆë‹¤.'\n5. ì‹ ê²½ì„±: 'ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë°›ìœ¼ë©´ ì ì„ ì˜ ëª» ìê³  ì˜ˆë¯¼í•´ì ¸ìš”. ì£¼ë¡œ í˜¼ì ì¡°ìš©í•œ ìŒì•…ì„ ë“£ê±°ë‚˜ ì‚°ì±…í•˜ë©´ì„œ í’€ë ¤ê³  í•©ë‹ˆë‹¤.'"
    },
    {
      "role": "assistant",
      "content": "## Big5 ì‹¬ë¦¬ ë¶„ì„ ë¦¬í¬íŠ¸\n\në‹¹ì‹ ì˜ ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„í•œ ì„±ê²© íŠ¹ì„±ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\n### ê°œë°©ì„± (Openness): ë†’ìŒ\n'ìƒˆë¡œìš´ ë ˆì‹œí”¼ ë°°ìš°ê¸°'ì—ì„œ ì¦ê±°ì›€ì„ ì–»ëŠ”ë‹¤ëŠ” ì ì—ì„œ, ë‹¹ì‹ ì€ ìƒˆë¡œìš´ ê²½í—˜ê³¼ ì§€ì‹ì— ëŒ€í•´ ë§¤ìš° ê°œë°©ì ì¸ ì„±í–¥ì„ ë³´ì…ë‹ˆë‹¤. í˜¸ê¸°ì‹¬ì´ ë§ê³  ë°°ì›€ì„ ì¦ê¸°ëŠ” ëª¨ìŠµì´ ëšœë ·í•©ë‹ˆë‹¤.\n\n### ì„±ì‹¤ì„± (Conscientiousness): ë§¤ìš° ë†’ìŒ\n'ê³„íš'ê³¼ 'ì²´í¬ë¦¬ìŠ¤íŠ¸'ë¥¼ í†µí•´ 'ë¿Œë“¯í•¨'ê³¼ 'ìì‹ ê°'ì„ ì–»ëŠ” ëª¨ìŠµì€ ë§¤ìš° ë†’ì€ ì„±ì‹¤ì„±ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì±…ì„ê°ì´ ê°•í•˜ê³  ëª©í‘œ ì§€í–¥ì ì´ë©° ì²´ê³„ì ì¸ ì„±ê²©ì…ë‹ˆë‹¤.\n\n### ì™¸í–¥ì„± (Extraversion): ë‚®ìŒ (ë‚´í–¥ì„±)\nì—ë„ˆì§€ë¥¼ 'í˜¼ì ìˆì„ ë•Œ' ì–»ëŠ”ë‹¤ê³  ëª…í™•íˆ ë‹µí•œ ë¶€ë¶„ì—ì„œ ë‚´í–¥ì ì¸ ì„±í–¥ì´ ê°•í•˜ê²Œ ë“œëŸ¬ë‚©ë‹ˆë‹¤. ì‚¬íšŒì  êµë¥˜ë³´ë‹¤ëŠ” ê°œì¸ì ì¸ ì‹œê°„ì„ í†µí•´ ì¬ì¶©ì „í•˜ëŠ” íƒ€ì…ì…ë‹ˆë‹¤.\n\n### ìš°í˜¸ì„± (Agreeableness): ë†’ìŒ\níƒ€ì¸ì˜ ë§ì„ 'ëê¹Œì§€ ë“¤ì–´ì£¼ê³  ê³µê°'í•˜ë ¤ëŠ” íƒœë„ëŠ” ë†’ì€ ìš°í˜¸ì„±ì„ ì‹œì‚¬í•©ë‹ˆë‹¤. íƒ€ì¸ì—ê²Œ í˜‘ì¡°ì ì´ë©° ê³µê° ëŠ¥ë ¥ì´ ë›°ì–´ë‚œ ë”°ëœ»í•œ ì„±ê²©ì…ë‹ˆë‹¤.\n\n### ì‹ ê²½ì„± (Neuroticism): ë‹¤ì†Œ ë†’ìŒ\nìŠ¤íŠ¸ë ˆìŠ¤ ìƒí™©ì—ì„œ 'ì ì„ ëª» ìê³  ì˜ˆë¯¼í•´ì§€ëŠ”' ë°˜ì‘ì€ ì •ì„œì  ë¯¼ê°ì„±(ì‹ ê²½ì„±)ì´ ë‹¤ì†Œ ë†’ì„ ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë‹¤ë§Œ, 'ìŒì•… ë“£ê¸°', 'ì‚°ì±…' ë“± ê±´ê°•í•œ ë°©ì‹ìœ¼ë¡œ í•´ì†Œí•˜ë ¤ ë…¸ë ¥í•˜ëŠ” ì ë„ ê¸ì •ì ì…ë‹ˆë‹¤.\n\n## ì¢…í•© ì˜ê²¬\n\në‹¹ì‹ ì€ ì„±ì‹¤í•˜ê³  ì±…ì„ê°ì´ ê°•í•˜ë©°, ìƒˆë¡œìš´ ì§€ì‹ì— ì—´ë ¤ìˆëŠ” 'í•™ìŠµì' ìœ í˜•ì…ë‹ˆë‹¤. ë‚´í–¥ì ì¸ ì„±í–¥ ë•ë¶„ì— ê¹Šì´ ìˆëŠ” í•™ìŠµì´ ê°€ëŠ¥í•˜ë©°, íƒ€ì¸ì— ëŒ€í•œ ê³µê° ëŠ¥ë ¥ë„ ë›°ì–´ë‚©ë‹ˆë‹¤. ì •ì„œì  ë¯¼ê°ì„±ì´ ìˆì§€ë§Œ, ê±´ê°•í•œ ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œ ë°©ì‹ì„ ì•Œê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ íŠ¹ì„±ë“¤ì˜ ì¡°í™”ê°€ ë‹¹ì‹ ì„ ì‹ ë¢°í•  ìˆ˜ ìˆê³  í†µì°°ë ¥ ìˆëŠ” ì‚¬ëŒìœ¼ë¡œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤."
    }
  ]
}
```

#### 2.2 ë°ì´í„° ìƒì„± ì „ëµ

**1ë‹¨ê³„: ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œ ë°ì´í„° ìƒì„± (100ê±´)**
```python
import json
from datetime import datetime

def create_golden_dataset():
    """ìˆ˜ë™ìœ¼ë¡œ ì œì‘í•œ ìµœê³  í’ˆì§ˆì˜ í•™ìŠµ ë°ì´í„°"""

    golden_examples = [
        {
            "user_answers": {
                "openness": "ìƒˆë¡œìš´ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ ë°°ìš°ëŠ” ê±¸ ì¢‹ì•„í•´ìš”. ë§¤ë‹¬ ìƒˆë¡œìš´ ê¸°ìˆ  ìŠ¤íƒì„ ì‹œë„í•´ë´ìš”.",
                "conscientiousness": "í”„ë¡œì íŠ¸ ê³„íšì„ ì§œê³  ê·¸ëŒ€ë¡œ ì‹¤í–‰í•˜ëŠ” ê±¸ ì¦ê²¨ìš”. ì¹¸ë°˜ë³´ë“œë¡œ ì§„í–‰ ìƒí™©ì„ ê´€ë¦¬í•´ìš”.",
                "extraversion": "ì‚¬ëŒë“¤ê³¼ í•¨ê»˜ í˜‘ì—…í•˜ëŠ” ê±´ ì¢‹ì§€ë§Œ, ì½”ë“œ ë¦¬ë·°ëŠ” í˜¼ì ì¡°ìš©íˆ í•  ë•Œ ë” ì§‘ì¤‘ì´ ì˜ë¼ìš”.",
                "agreeableness": "íŒ€ì›ì´ ì–´ë ¤ì›Œí•˜ë©´ ë¨¼ì € ë‹¤ê°€ê°€ì„œ ë„ì™€ì£¼ë ¤ê³  í•´ìš”. í•¨ê»˜ ì„±ì¥í•˜ëŠ” ê²Œ ì¤‘ìš”í•˜ë‹¤ê³  ìƒê°í•´ìš”.",
                "neuroticism": "ë°ë“œë¼ì¸ì´ ë‹¤ê°€ì˜¤ë©´ ë¶ˆì•ˆí•´ì„œ ì ì„ ì„¤ì³ìš”. ëª…ìƒì´ë‚˜ ìš´ë™ìœ¼ë¡œ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ í’€ì–´ìš”."
            },
            "golden_report": """## Big5 ì‹¬ë¦¬ ë¶„ì„ ë¦¬í¬íŠ¸

ë‹¹ì‹ ì˜ ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„í•œ ì„±ê²© íŠ¹ì„±ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

### ê°œë°©ì„± (Openness): ë§¤ìš° ë†’ìŒ
'ìƒˆë¡œìš´ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ ë°°ìš°ê¸°'ì™€ 'ë§¤ë‹¬ ìƒˆë¡œìš´ ê¸°ìˆ  ìŠ¤íƒ ì‹œë„'ì—ì„œ ë³´ì—¬ì£¼ëŠ” íƒœë„ëŠ” ë§¤ìš° ë†’ì€ ê°œë°©ì„±ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ê¸°ìˆ ì— ëŒ€í•œ í˜¸ê¸°ì‹¬ê³¼ í•™ìŠµ ì˜ì§€ê°€ ë›°ì–´ë‚˜ë©°, ë³€í™”ì™€ ìƒˆë¡œì›€ì„ ì ê·¹ì ìœ¼ë¡œ ìˆ˜ìš©í•˜ëŠ” ëª¨ìŠµì…ë‹ˆë‹¤.

### ì„±ì‹¤ì„± (Conscientiousness): ë§¤ìš° ë†’ìŒ
'í”„ë¡œì íŠ¸ ê³„íš'ê³¼ 'ì¹¸ë°˜ë³´ë“œ ê´€ë¦¬'ì—ì„œ ë“œëŸ¬ë‚˜ëŠ” ì²´ê³„ì ì¸ ì ‘ê·¼ ë°©ì‹ì€ ë§¤ìš° ë†’ì€ ì„±ì‹¤ì„±ì˜ íŠ¹ì§•ì…ë‹ˆë‹¤. ëª©í‘œ ì§€í–¥ì ì´ê³  ì±…ì„ê°ì´ ê°•í•˜ë©°, ìì‹ ì˜ ì—…ë¬´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ëŠ” ëŠ¥ë ¥ì´ ë›°ì–´ë‚©ë‹ˆë‹¤.

### ì™¸í–¥ì„± (Extraversion): ì¤‘ê°„
'í˜‘ì—… ì¦ê¹€'ê³¼ 'í˜¼ì ì½”ë“œ ë¦¬ë·°'ì˜ ëª¨ìŠµì€ ì‚¬íšŒì  ìƒí˜¸ì‘ìš©ê³¼ ê°œì¸ì  ì§‘ì¤‘ ì‚¬ì´ì˜ ê· í˜•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ìƒí™©ì— ë”°ë¼ ì™¸í–¥ì , ë‚´í–¥ì  íŠ¹ì„±ì„ ìœ ì—°í•˜ê²Œ ì¡°ì ˆí•  ìˆ˜ ìˆëŠ” ê· í˜• ì¡íŒ ì„±í–¥ì…ë‹ˆë‹¤.

### ìš°í˜¸ì„± (Agreeableness): ë†’ìŒ
'íŒ€ì› ë„ì™€ì£¼ê¸°'ì™€ 'í•¨ê»˜ ì„±ì¥'ì˜ ê°€ì¹˜ê´€ì—ì„œ ë†’ì€ ìš°í˜¸ì„±ì´ ì—¿ë³´ì…ë‹ˆë‹¤. íƒ€ì¸ì˜ ì„±ì¥ì„ ì§€ì›í•˜ê³  í˜‘ë ¥ì ì¸ ê´€ê³„ë¥¼ ì¤‘ì‹œí•˜ëŠ” ë”°ëœ»í•˜ê³  ì´íƒ€ì ì¸ ì„±ê²©ì…ë‹ˆë‹¤.

### ì‹ ê²½ì„± (Neuroticism): ì¤‘ê°„
'ë°ë“œë¼ì¸ ë¶ˆì•ˆ'ê³¼ 'ëª…ìƒ/ìš´ë™ìœ¼ë¡œ í•´ì†Œ'ì˜ íŒ¨í„´ì€ ì •ì„œì  ë¯¼ê°ì„±ì´ ìˆì§€ë§Œ, ê±´ê°•í•œ ëŒ€ì²˜ ë°©ì‹ì„ ì•Œê³  ìˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ìŠ¤íŠ¸ë ˆìŠ¤ ìƒí™©ì—ì„œ ë¶ˆì•ˆì„ ëŠë¼ë˜, ì´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê´€ë¦¬í•˜ëŠ” ëŠ¥ë ¥ì„ ê°–ì¶”ê³  ìˆìŠµë‹ˆë‹¤.

## ì¢…í•© ì˜ê²¬

ë‹¹ì‹ ì€ í•™ìŠµì— ëŒ€í•œ ì—´ì •ê³¼ ì²´ê³„ì ì¸ ì‹¤í–‰ë ¥ì„ ê²¸ë¹„í•œ 'ì„±ì¥í˜• ê°œë°œì'ì…ë‹ˆë‹¤. ìƒˆë¡œìš´ ê¸°ìˆ ì„ ì ê·¹ì ìœ¼ë¡œ íƒêµ¬í•˜ë©´ì„œë„, í”„ë¡œì íŠ¸ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬í•˜ëŠ” ëŠ¥ë ¥ì´ ë›°ì–´ë‚©ë‹ˆë‹¤. íŒ€ì›ë“¤ê³¼ì˜ í˜‘ë ¥ì„ ì¤‘ì‹œí•˜ë©°, ìŠ¤íŠ¸ë ˆìŠ¤ ìƒí™©ì—ì„œë„ ê±´ê°•í•œ ëŒ€ì²˜ ë°©ì‹ì„ ì•Œê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ íŠ¹ì„±ë“¤ì˜ ì¡°í™”ê°€ ë‹¹ì‹ ì„ ê¸°ìˆ ì ìœ¼ë¡œ ë›°ì–´ë‚˜ë©´ì„œë„ íŒ€ì›Œí¬ì— ê¸°ì—¬í•˜ëŠ” ê°œë°œìë¡œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤."""
        }
        # ... 99ê°œì˜ ì˜ˆì œ ì¶”ê°€
    ]

    return golden_examples

# ê³¨ë“œ ë°ì´í„°ì…‹ ìƒì„± ë° ì €ì¥
golden_data = create_golden_dataset()
with open('golden_dataset.jsonl', 'w', encoding='utf-8') as f:
    for example in golden_data:
        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ Big5 ì‹¬ë¦¬í•™ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¬ìš©ìì˜ ë‹µë³€ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ ì‹¬ë¦¬ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ 5ê°€ì§€ ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ, ê° íŠ¹ì„±(ê°œë°©ì„±, ì„±ì‹¤ì„±, ì™¸í–¥ì„±, ìš°í˜¸ì„±, ì‹ ê²½ì„±)ì„ ë¶„ì„í•˜ê³  ê¸ì •ì ì´ë©° í†µì°°ë ¥ ìˆëŠ” ì¢…í•© ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ì ˆëŒ€ ì˜í•™ì  ì§„ë‹¨ì„ ë‚´ë¦¬ì§€ ë§ˆì„¸ìš”."},
            {"role": "user", "content": format_user_input(example["user_answers"])},
            {"role": "assistant", "content": example["golden_report"]}
        ]
        f.write(json.dumps({"messages": messages}, ensure_ascii=False) + '\n')
```

**2ë‹¨ê³„: í•©ì„± ë°ì´í„° ìƒì„± (500-1000ê±´)**
```python
import openai
from typing import List, Dict

def generate_synthetic_data(base_examples: List[Dict], count: int = 500):
    """ê³ í’ˆì§ˆ ê¸°ë°˜ ë°ì´í„°ë¥¼ í™œìš©í•œ í•©ì„± ë°ì´í„° ìƒì„±"""

    synthetic_data = []

    for i in range(count):
        # ê¸°ì¡´ ì˜ˆì œì—ì„œ íŒ¨í„´ í•™ìŠµ
        base_example = base_examples[i % len(base_examples)]

        # í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
        prompt = f"""
ë‹¤ìŒ Big5 ì‹¬ë¦¬ ë¶„ì„ ì˜ˆì œë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ìƒˆë¡œìš´ ì‚¬ìš©ì ë‹µë³€ê³¼ ê·¸ì— ëŒ€í•œ ì „ë¬¸ì  ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ê¸°ì¤€ ì˜ˆì œ:
{format_example(base_example)}

ìš”êµ¬ì‚¬í•­:
1. ê° Big5 íŠ¹ì„±ì— ëŒ€í•´ ìƒˆë¡œìš´ êµ¬ì²´ì ì¸ ë‹µë³€ ìƒì„±
2. ì „ë¬¸ì„±, ê³µê° ëŠ¥ë ¥, í†µì°°ë ¥, ì¡°ì‹¬ì„±, êµ¬ì¡°í™”ëœ í˜ë¥´ì†Œë‚˜ ìœ ì§€
3. ë‹¨ì •ì  ì§„ë‹¨ í‘œí˜„ ê¸ˆì§€ ("~ì…ë‹ˆë‹¤" ëŒ€ì‹  "~í•œ ì„±í–¥ì„ ë³´ì…ë‹ˆë‹¤")
4. ê¸ì •ì ì´ê³  ìˆ˜ìš©ì ì¸ í†¤ ìœ ì§€
5. ê° íŠ¹ì„±ë³„ ë¶„ì„ê³¼ ì¢…í•© ì˜ê²¬ í¬í•¨

JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
"""

        try:
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7
            )

            synthetic_example = json.loads(response.choices[0].message.content)
            synthetic_data.append(synthetic_example)

            if (i + 1) % 50 == 0:
                print(f"Generated {i + 1}/{count} synthetic examples")

        except Exception as e:
            print(f"Error generating example {i}: {e}")
            continue

    return synthetic_data

# í•©ì„± ë°ì´í„° ìƒì„±
synthetic_data = generate_synthetic_data(golden_data, count=800)
```

#### 2.3 ë°ì´í„° ê²€ìˆ˜ ë° ì •ì œ
```python
def validate_dataset(dataset: List[Dict]):
    """ë°ì´í„°ì…‹ í’ˆì§ˆ ê²€ìˆ˜"""

    validation_results = {
        "total_examples": len(dataset),
        "valid_examples": 0,
        "issues": []
    }

    for i, example in enumerate(dataset):
        issues = []

        # 1. êµ¬ì¡° ê²€ì¦
        if "messages" not in example:
            issues.append("Missing 'messages' field")

        # 2. ì—­í•  ê²€ì¦
        roles = [msg["role"] for msg in example.get("messages", [])]
        if set(roles) != {"system", "user", "assistant"}:
            issues.append("Invalid role structure")

        # 3. í˜ë¥´ì†Œë‚˜ ê²€ì¦
        assistant_content = ""
        for msg in example.get("messages", []):
            if msg["role"] == "assistant":
                assistant_content = msg["content"]
                break

        # ê¸ˆì§€ í‘œí˜„ ê²€ì‚¬
        forbidden_patterns = [
            "ì§„ë‹¨", "ì…ë‹ˆë‹¤", "í™•ì‹¤í•©ë‹ˆë‹¤", "ë¶„ëª…í•©ë‹ˆë‹¤",
            "ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤", "ë¹„ì •ìƒì…ë‹ˆë‹¤"
        ]

        for pattern in forbidden_patterns:
            if pattern in assistant_content:
                issues.append(f"Contains forbidden pattern: {pattern}")

        # í•„ìˆ˜ êµ¬ì¡° ê²€ì‚¬
        required_sections = ["ê°œë°©ì„±", "ì„±ì‹¤ì„±", "ì™¸í–¥ì„±", "ìš°í˜¸ì„±", "ì‹ ê²½ì„±", "ì¢…í•© ì˜ê²¬"]
        for section in required_sections:
            if section not in assistant_content:
                issues.append(f"Missing required section: {section}")

        if not issues:
            validation_results["valid_examples"] += 1
        else:
            validation_results["issues"].append({
                "example_id": i,
                "issues": issues
            })

    return validation_results

# ë°ì´í„°ì…‹ ê²€ìˆ˜ ì‹¤í–‰
validation_results = validate_dataset(final_dataset)
print(f"Validation Results:")
print(f"Total: {validation_results['total_examples']}")
print(f"Valid: {validation_results['valid_examples']}")
print(f"Invalid: {validation_results['total_examples'] - validation_results['valid_examples']}")
```

---

### 3ë‹¨ê³„: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ê¸°ë³¸ í…ŒìŠ¤íŠ¸ (1ì¼)

#### 3.1 Qwen2-1.5B ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

def download_qwen_model():
    """Qwen2-1.5B ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ê¸°ë³¸ ì„¤ì •"""

    model_name = "Qwen/Qwen2-1.5B"

    print("Downloading Qwen2-1.5B model...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True,
        padding_side="left"
    )

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )

    # íŒ¨ë”© í† í° ì„¤ì •
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print("Model downloaded successfully!")
    return model, tokenizer

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
model, tokenizer = download_qwen_model()
```

#### 3.2 ê¸°ë³¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸
```python
def test_basic_inference(model, tokenizer):
    """ëª¨ë¸ ê¸°ë³¸ ì¶”ë¡  ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸"""

    test_prompt = """ë‹¹ì‹ ì€ Big5 ì‹¬ë¦¬ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì‚¬ìš©ì ë‹µë³€ì— ëŒ€í•´ ê°„ë‹¨íˆ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ë‹µë³€:
1. ê°œë°©ì„±: "ìƒˆë¡œìš´ ê²ƒì„ ë°°ìš°ëŠ” ê±¸ ì¢‹ì•„í•´ìš”"
2. ì„±ì‹¤ì„±: "ê³„íšì„ ì„¸ìš°ê³  ì§€í‚¤ëŠ” ê²Œ ì¤‘ìš”í•´ìš”"

ë¶„ì„:"""

    inputs = tokenizer(test_prompt, return_tensors="pt", padding=True)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=200,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print("Basic Inference Test Result:")
    print(response)

    return response

# ê¸°ë³¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸
test_response = test_basic_inference(model, tokenizer)
```

---

### 4ë‹¨ê³„: íŒŒì¸íŠœë‹ ì‹¤í–‰ (3-7ì¼)

#### 4.1 LLaMA-Factory ì‚¬ìš© (ê°€ì¥ ì‰¬ìš´ ë°©ë²•)

**LLaMA-Factory ì„¤ì¹˜ ë° ì„¤ì •**:
```bash
# 1. LLaMA-Factory í´ë¡ 
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory

# 2. ì„¤ì¹˜
pip install -e .[torch,metrics]

# 3. ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p data/big5_dataset
```

**ë°ì´í„°ì…‹ ì„¤ì •**:
```yaml
# dataset_info.yaml ìˆ˜ì •
big5_dataset:
  path_or_name: data/big5_dataset
  prompt_template: qwen
  formatting: alpaca
```

**íŒŒì¸íŠœë‹ ì‹¤í–‰**:
```bash
# QLoRA íŒŒì¸íŠœë‹ ì‹¤í–‰
llamafactory-cli train \
  --model_name_or_path qwen2-1.5b \
  --dataset big5_dataset \
  --template qwen \
  --finetuning_type lora \
  --lora_target q_proj,v_proj \
  --lora_rank 64 \
  --lora_alpha 128 \
  --q_lora true \
  --output_dir saves/qwen2-1.5b-big5 \
  --per_device_train_batch_size 4 \
  --gradient_accumulation_steps 4 \
  --lr_scheduler_type cosine \
  --logging_steps 10 \
  --save_steps 500 \
  --learning_rate 1e-4 \
  --num_train_epochs 3 \
  --plot_loss true \
  --bf16 true \
  --dataset_dir data \
  --preprocessing_num_workers 16
```

#### 4.2 ì§ì ‘ íŠœë‹ (ë” ë§ì€ ì œì–´)

**ì»¤ìŠ¤í…€ íŠ¸ë ˆì´ë‹ ìŠ¤í¬ë¦½íŠ¸**:
```python
import torch
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    DataCollatorForSeq2Seq
)
from peft import LoraConfig, get_peft_model, TaskType
from trl import SFTTrainer

class QwenBig5Trainer:
    def __init__(self, model_name="Qwen/Qwen2-1.5B"):
        self.model_name = model_name
        self.tokenizer = None
        self.model = None

    def setup_model_and_tokenizer(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ì„¤ì •"""

        print("Setting up model and tokenizer...")

        # í† í¬ë‚˜ì´ì €
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True,
            padding_side="left"
        )

        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # ëª¨ë¸
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )

        # LoRA ì„¤ì •
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=64,
            lora_alpha=128,
            lora_dropout=0.1,
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj"]
        )

        self.model = get_peft_model(self.model, lora_config)
        self.model.print_trainable_parameters()

    def load_and_preprocess_dataset(self, dataset_path):
        """ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬"""

        print(f"Loading dataset from {dataset_path}...")

        dataset = load_dataset("json", data_files=dataset_path, split="train")

        def formatting_prompts_func(examples):
            output_texts = []
            for i in range(len(examples["messages"])):
                messages = examples["messages"][i]

                # ChatML í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                formatted_text = ""
                for message in messages:
                    if message["role"] == "system":
                        formatted_text += f"<|im_start|>system\n{message['content']}<|im_end|>\n"
                    elif message["role"] == "user":
                        formatted_text += f"<|im_start|>user\n{message['content']}<|im_end|>\n"
                    elif message["role"] == "assistant":
                        formatted_text += f"<|im_start|>assistant\n{message['content']}<|im_end|>"

                output_texts.append(formatted_text)

            return {"text": output_texts}

        dataset = dataset.map(formatting_prompts_func, batched=True)
        return dataset

    def train(self, dataset_path, output_dir="./qwen2-1.5b-big5-tuned"):
        """íŒŒì¸íŠœë‹ ì‹¤í–‰"""

        # ëª¨ë¸ ì„¤ì •
        self.setup_model_and_tokenizer()

        # ë°ì´í„°ì…‹ ë¡œë“œ
        dataset = self.load_and_preprocess_dataset(dataset_path)

        # íŠ¸ë ˆì´ë‹ ì¸ì
        training_args = TrainingArguments(
            output_dir=output_dir,
            per_device_train_batch_size=4,
            gradient_accumulation_steps=4,
            learning_rate=1e-4,
            logging_steps=10,
            save_steps=500,
            num_train_epochs=3,
            lr_scheduler_type="cosine",
            warmup_steps=100,
            fp16=True,
            dataloader_num_workers=4,
            save_total_limit=3,
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            report_to="tensorboard"
        )

        # ë°ì´í„° ì½œë ˆì´í„°
        data_collator = DataCollatorForSeq2Seq(
            tokenizer=self.tokenizer,
            model=self.model,
            padding=True,
            return_tensors="pt"
        )

        # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
        trainer = SFTTrainer(
            model=self.model,
            args=training_args,
            train_dataset=dataset,
            data_collator=data_collator,
            tokenizer=self.tokenizer,
            max_seq_length=2048,
            dataset_text_field="text",
            packing=False
        )

        # íŠ¸ë ˆì´ë‹ ì‹¤í–‰
        print("Starting training...")
        trainer.train()

        # ëª¨ë¸ ì €ì¥
        trainer.save_model(output_dir)
        self.tokenizer.save_pretrained(output_dir)

        print(f"Training completed! Model saved to {output_dir}")
        return trainer

# íŠ¸ë ˆì´ë„ˆ ì‹¤í–‰
trainer = QwenBig5Trainer()
trainer.train("final_dataset.jsonl")
```

#### 4.3 íŠ¸ë ˆì´ë‹ ëª¨ë‹ˆí„°ë§
```python
import wandb
from torch.utils.tensorboard import SummaryWriter

# WandB ì´ˆê¸°í™”
wandb.init(
    project="qwen2-big5-tuning",
    config={
        "model": "Qwen2-1.5B",
        "dataset_size": len(dataset),
        "epochs": 3,
        "batch_size": 4,
        "learning_rate": 1e-4
    }
)

# TensorBoard ë¡œê¹…
writer = SummaryWriter("./logs/big5-tuning")

def log_training_metrics(trainer, epoch, loss):
    """íŠ¸ë ˆì´ë‹ ë©”íŠ¸ë¦­ ë¡œê¹…"""

    # WandB ë¡œê¹…
    wandb.log({
        "epoch": epoch,
        "train_loss": loss,
        "learning_rate": trainer.args.learning_rate
    })

    # TensorBoard ë¡œê¹…
    writer.add_scalar("Loss/Train", loss, epoch)
    writer.add_scalar("Learning_Rate", trainer.args.learning_rate, epoch)
```

---

### 5ë‹¨ê³„: ëª¨ë¸ ë³‘í•© ë° í…ŒìŠ¤íŠ¸ (1-2ì¼)

#### 5.1 LoRA ì–´ëŒ‘í„° ë³‘í•©
```python
from peft import PeftModel
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

def merge_lora_adapter(base_model_path, lora_adapter_path, output_path):
    """LoRA ì–´ëŒ‘í„°ë¥¼ ì›ë³¸ ëª¨ë¸ì— ë³‘í•©"""

    print("Loading base model...")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )

    print("Loading LoRA adapter...")
    model = PeftModel.from_pretrained(base_model, lora_adapter_path)

    print("Merging adapter...")
    merged_model = model.merge_and_unload()

    print("Saving merged model...")
    merged_model.save_pretrained(output_path)

    # í† í¬ë‚˜ì´ì €ë„ ì €ì¥
    tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
    tokenizer.save_pretrained(output_path)

    print(f"Merged model saved to {output_path}")
    return merged_model

# ë³‘í•© ì‹¤í–‰
merge_lora_adapter(
    base_model_path="Qwen/Qwen2-1.5B",
    lora_adapter_path="./saves/qwen2-1.5b-big5",
    output_path="./merged-qwen2-1.5b-big5"
)
```

#### 5.2 íŠœë‹ ê²°ê³¼ í…ŒìŠ¤íŠ¸
```python
def test_tuned_model(model_path, test_cases):
    """íŠœë‹ëœ ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""

    print("Loading tuned model...")
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )

    results = []

    for i, test_case in enumerate(test_cases):
        print(f"Testing case {i+1}/{len(test_cases)}")

        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = build_big5_prompt(test_case["answers"])

        # ì¶”ë¡ 
        inputs = tokenizer(prompt, return_tensors="pt", padding=True)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=800,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # ì‘ë‹µ ë¶„ì„
        analysis = analyze_response(response, test_case["expected_personas"])

        results.append({
            "test_case": test_case,
            "response": response,
            "analysis": analysis
        })

        print(f"âœ“ Case {i+1} completed")

    return results

# í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì •ì˜
test_cases = [
    {
        "answers": {
            "openness": "ìƒˆë¡œìš´ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¥¼ ë°°ìš°ëŠ” ê±¸ ì¢‹ì•„í•´ìš”",
            "conscientiousness": "ê³„íšì„ ì„¸ìš°ê³  ê·¸ëŒ€ë¡œ ì‹¤í–‰í•˜ëŠ” ê²Œ ì¤‘ìš”í•´ìš”",
            "extraversion": "ì‚¬ëŒë“¤ê³¼ í•¨ê»˜ ì¼í•˜ëŠ” ê±¸ ì¢‹ì•„í•˜ì§€ë§Œ, í˜¼ì ì¼í•  ë•Œë„ ì¢‹ì•„ìš”",
            "agreeableness": "íŒ€ì›ë“¤ì„ ë•ëŠ” ê±¸ ì¦ê²¨ìš”",
            "neuroticism": "ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë°›ìœ¼ë©´ ìš´ë™ìœ¼ë¡œ í’€ì–´ìš”"
        },
        "expected_personas": ["openness_high", "conscientiousness_high"]
    }
    # ... ë” ë§ì€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
]

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
test_results = test_tuned_model("./merged-qwen2-1.5b-big5", test_cases)
```

#### 5.3 ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
```python
def evaluate_model_performance(test_results):
    """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë° ë¦¬í¬íŠ¸ ìƒì„±"""

    evaluation_metrics = {
        "total_tests": len(test_results),
        "persona_compliance": 0,
        "structure_compliance": 0,
        "safety_compliance": 0,
        "average_response_length": 0,
        "issues": []
    }

    total_compliance_score = 0
    total_length = 0

    for result in test_results:
        response = result["response"]
        analysis = result["analysis"]

        # í˜ë¥´ì†Œë‚˜ ì¤€ìˆ˜ìœ¨
        persona_score = analysis.get("persona_compliance_score", 0)
        total_compliance_score += persona_score

        # êµ¬ì¡° ì¤€ìˆ˜ìœ¨
        structure_score = analysis.get("structure_compliance_score", 0)
        total_compliance_score += structure_score

        # ì•ˆì „ì„± ì¤€ìˆ˜ìœ¨
        safety_score = analysis.get("safety_compliance_score", 0)
        total_compliance_score += safety_score

        # ì‘ë‹µ ê¸¸ì´
        total_length += len(response)

        # ì´ìŠˆ ìˆ˜ì§‘
        if analysis.get("issues"):
            evaluation_metrics["issues"].extend(analysis["issues"])

    # í‰ê·  ê³„ì‚°
    evaluation_metrics["persona_compliance"] = total_compliance_score / (len(test_results) * 3)
    evaluation_metrics["average_response_length"] = total_length / len(test_results)

    # ê²°ê³¼ ì¶œë ¥
    print("=== Model Performance Evaluation ===")
    print(f"Total Tests: {evaluation_metrics['total_tests']}")
    print(f"Average Compliance Score: {evaluation_metrics['persona_compliance']:.2%}")
    print(f"Average Response Length: {evaluation_metrics['average_response_length']:.0f} chars")

    if evaluation_metrics["issues"]:
        print("\nIssues Found:")
        for issue in evaluation_metrics["issues"][:10]:  # ìƒìœ„ 10ê°œë§Œ í‘œì‹œ
            print(f"  - {issue}")

    return evaluation_metrics

# ì„±ëŠ¥ í‰ê°€ ì‹¤í–‰
performance_report = evaluate_model_performance(test_results)
```

---

### 6ë‹¨ê³„: GGUF ë³€í™˜ ë° ì–‘ìí™” (1-2ì¼)

#### 6.1 llama.cpp í™˜ê²½ ì„¤ì •
```bash
# 1. llama.cpp í´ë¡ 
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp

# 2. Python ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# 3. ì»´íŒŒì¼ (Linux/macOS)
make clean
make LLAMA_CUBLAS=1 -j$(nproc)

# 4. ì„¤ì¹˜ í™•ì¸
./main --help
```

#### 6.2 GGUF ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸
```python
import subprocess
import os

def convert_to_gguf(hf_model_path, output_path, model_type="f16"):
    """Hugging Face ëª¨ë¸ì„ GGUF í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""

    print(f"Converting {hf_model_path} to GGUF format...")

    # ë³€í™˜ ëª…ë ¹ì–´
    cmd = [
        "python", "convert.py",
        hf_model_path,
        "--outfile", output_path,
        "--outtype", model_type
    ]

    try:
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)
        print(f"âœ“ Conversion completed: {output_path}")
        return True
    except subprocess.CalledProcessError as e:
        print(f"âœ— Conversion failed: {e}")
        print(f"Error output: {e.stderr}")
        return False

def quantize_gguf(input_path, output_path, quant_type="Q4_K_M"):
    """GGUF ëª¨ë¸ ì–‘ìí™”"""

    print(f"Quantizing {input_path} to {quant_type}...")

    # ì–‘ìí™” ëª…ë ¹ì–´
    cmd = [
        "./quantize",
        input_path,
        output_path,
        quant_type
    ]

    try:
        result = subprocess.run(cmd, check=True, capture_output=True, text=True)

        # íŒŒì¼ í¬ê¸° í™•ì¸
        original_size = os.path.getsize(input_path)
        quantized_size = os.path.getsize(output_path)
        compression_ratio = (1 - quantized_size / original_size) * 100

        print(f"âœ“ Quantization completed: {output_path}")
        print(f"  Original size: {original_size / (1024**3):.2f} GB")
        print(f"  Quantized size: {quantized_size / (1024**3):.2f} GB")
        print(f"  Compression ratio: {compression_ratio:.1f}%")

        return True
    except subprocess.CalledProcessError as e:
        print(f"âœ— Quantization failed: {e}")
        print(f"Error output: {e.stderr}")
        return False

# ë³€í™˜ ë° ì–‘ìí™” ì‹¤í–‰
if __name__ == "__main__":
    # ê²½ë¡œ ì„¤ì •
    hf_model_path = "../merged-qwen2-1.5b-big5"
    fp16_output = "../qwen2-1.5b-big5.fp16.gguf"
    quantized_output = "../qwen2-1.5b-big5.Q4_K_M.gguf"

    # 1. FP16ìœ¼ë¡œ ë³€í™˜
    if convert_to_gguf(hf_model_path, fp16_output, "f16"):
        # 2. 4ë¹„íŠ¸ ì–‘ìí™”
        if quantize_gguf(fp16_output, quantized_output, "Q4_K_M"):
            print("ğŸ‰ GGUF conversion and quantization completed successfully!")
        else:
            print("âŒ Quantization failed")
    else:
        print("âŒ Conversion failed")
```

#### 6.3 ì–‘ìí™” ì˜µì…˜ ìµœì í™”
```python
def compare_quantization_methods(fp16_model_path):
    """ë‹¤ì–‘í•œ ì–‘ìí™” ë°©ë²• ë¹„êµ í…ŒìŠ¤íŠ¸"""

    quant_types = [
        "Q4_0",    # 4ë¹„íŠ¸ ê¸°ë³¸ (ë¹ ë¦„, í’ˆì§ˆ ë‚®ìŒ)
        "Q4_1",    # 4ë¹„íŠ¸ ê°œì„ í˜•
        "Q4_K_M",  # 4ë¹„íŠ¸ K-Quants ì¤‘ê°„ (ê¶Œì¥)
        "Q5_K_M",  # 5ë¹„íŠ¸ K-Quants ì¤‘ê°„ (ë” ì¢‹ì€ í’ˆì§ˆ)
        "Q8_0",    # 8ë¹„íŠ¸ (ìµœê³  í’ˆì§ˆ, í° íŒŒì¼)
    ]

    results = []

    for quant_type in quant_types:
        output_path = f"../qwen2-1.5b-big5.{quant_type}.gguf"

        print(f"\nTesting {quant_type}...")

        # ì–‘ìí™”
        if quantize_gguf(fp16_model_path, output_path, quant_type):
            # íŒŒì¼ í¬ê¸° í™•ì¸
            size_mb = os.path.getsize(output_path) / (1024**2)

            # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (ê°„ë‹¨í•œ ì¶”ë¡ )
            inference_time = test_inference_speed(output_path)

            results.append({
                "quant_type": quant_type,
                "size_mb": size_mb,
                "inference_time": inference_time,
                "path": output_path
            })

    # ê²°ê³¼ ë¹„êµ
    print("\n=== Quantization Comparison ===")
    for result in results:
        print(f"{result['quant_type']:8s} | {result['size_mb']:6.1f}MB | {result['inference_time']:6.2f}s")

    return results

def test_inference_speed(model_path, test_prompt="ì•ˆë…•í•˜ì„¸ìš”?"):
    """ì¶”ë¡  ì†ë„ í…ŒìŠ¤íŠ¸"""

    cmd = [
        "./main",
        "-m", model_path,
        "-p", test_prompt,
        "-n", "100",
        "--color"
    ]

    start_time = time.time()

    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
        end_time = time.time()

        return end_time - start_time
    except subprocess.TimeoutExpired:
        return 60.0  # íƒ€ì„ì•„ì›ƒ

# ì–‘ìí™” ë¹„êµ í…ŒìŠ¤íŠ¸
quantization_results = compare_quantization_methods("../qwen2-1.5b-big5.fp16.gguf")
```

---

### 7ë‹¨ê³„: ì•± ì—°ë™ í…ŒìŠ¤íŠ¸ (1-2ì¼)

#### 7.1 iOS ì•± ëª¨ë¸ í†µí•©
```typescript
// hooks/useOnDeviceSLLM.ts
import { useState, useEffect, useCallback } from 'react';
import { Platform } from 'react-native';

interface SLLMConfig {
  modelPath: string;
  contextLength: number;
  temperature: number;
}

export const useOnDeviceSLLM = (config: SLLMConfig) => {
  const [isLoading, setIsLoading] = useState(false);
  const [isGenerating, setIsGenerating] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [model, setModel] = useState<any>(null);

  // ëª¨ë¸ ë¡œë“œ
  const loadModel = useCallback(async () => {
    setIsLoading(true);
    setError(null);

    try {
      const startTime = Date.now();

      // ë„¤ì´í‹°ë¸Œ ëª¨ë“ˆ í˜¸ì¶œ
      const loadedModel = await SLLMNative.loadModel(config);

      const loadTime = Date.now() - startTime;
      console.log(`Model loaded in ${loadTime}ms`);

      // ì„±ëŠ¥ ê¸°ì¤€ í™•ì¸ (5ì´ˆ ì´ë‚´)
      if (loadTime > 5000) {
        console.warn(`Model loading took ${loadTime}ms, exceeding 5s target`);
      }

      setModel(loadedModel);

    } catch (err) {
      setError(err instanceof Error ? err.message : 'Model loading failed');
      console.error('Model loading error:', err);
    } finally {
      setIsLoading(false);
    }
  }, [config]);

  // í…ìŠ¤íŠ¸ ìƒì„±
  const generateText = useCallback(async (prompt: string): Promise<string> => {
    if (!model) {
      throw new Error('Model not loaded');
    }

    setIsGenerating(true);
    setError(null);

    try {
      const startTime = Date.now();

      const result = await SLLMNative.generateText(model, {
        prompt,
        maxTokens: 800,
        temperature: config.temperature,
        stopSequences: ["<|im_end|>"]
      });

      const inferenceTime = Date.now() - startTime;
      console.log(`Inference completed in ${inferenceTime}ms`);

      // ì„±ëŠ¥ ê¸°ì¤€ í™•ì¸ (15ì´ˆ ì´ë‚´)
      if (inferenceTime > 15000) {
        console.warn(`Inference took ${inferenceTime}ms, exceeding 15s target`);
      }

      return result;

    } catch (err) {
      const errorMessage = err instanceof Error ? err.message : 'Generation failed';
      setError(errorMessage);
      throw new Error(errorMessage);
    } finally {
      setIsGenerating(false);
    }
  }, [model, config.temperature]);

  // ë©”ëª¨ë¦¬ í•´ì œ
  const unloadModel = useCallback(async () => {
    if (model) {
      try {
        await SLLMNative.unloadModel(model);
        setModel(null);
      } catch (err) {
        console.error('Model unloading error:', err);
      }
    }
  }, [model]);

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
  useEffect(() => {
    return () => {
      unloadModel();
    };
  }, [unloadModel]);

  return {
    model,
    isLoading,
    isGenerating,
    error,
    loadModel,
    generateText,
    unloadModel
  };
};
```

#### 7.2 ë„¤ì´í‹°ë¸Œ ëª¨ë“ˆ ì—°ë™ (iOS)
```objective-c
// SLLMNative.mm
#import <Foundation/Foundation.h>
#import "llama.h"

@interface SLLMNative ()
@property (nonatomic, assign) llama_model* model;
@property (nonatomic, assign) llama_context* context;
@end

@implementation SLLMNative

+ (BOOL)requiresMainQueueSetup {
    return NO;
}

RCT_EXPORT_MODULE();

RCT_EXPORT_METHOD(loadModel:(NSDictionary *)config
                 resolver:(RCTPromiseResolveBlock)resolve
                 rejecter:(RCTPromiseRejectBlock)reject)
{
    dispatch_async(dispatch_get_global_queue(DISPATCH_QUEUE_PRIORITY_DEFAULT, 0), ^{
        NSString *modelPath = config[@"modelPath"];

        // ëª¨ë¸ íŒŒë¼ë¯¸í„° ì„¤ì •
        llama_model_params modelParams = llama_model_default_params();
        modelParams.n_gpu_layers = 0; // iOSì—ì„œëŠ” CPU ì‚¬ìš©

        // ëª¨ë¸ ë¡œë“œ
        NSString *fullPath = [[NSBundle mainBundle] pathForResource:modelPath ofType:nil];
        self.model = llama_load_model_from_file([fullPath UTF8String], modelParams);

        if (!self.model) {
            reject(@"MODEL_LOAD_ERROR", @"Failed to load model", nil);
            return;
        }

        // ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
        llama_context_params contextParams = llama_context_default_params();
        contextParams.n_ctx = [config[@"contextLength"] integerValue];
        contextParams.seed = 1234;

        self.context = llama_new_context_with_model(self.model, contextParams);

        if (!self.context) {
            llama_free_model(self.model);
            self.model = NULL;
            reject(@"CONTEXT_CREATE_ERROR", @"Failed to create context", nil);
            return;
        }

        resolve(@{@"modelId": @(1)}); // ì„±ê³µ ì‹œ ID ë°˜í™˜
    });
}

RCT_EXPORT_METHOD(generateText:(nonnull NSNumber *)modelId
                  config:(NSDictionary *)config
                  resolver:(RCTPromiseResolveBlock)resolve
                  rejecter:(RCTPromiseRejectBlock)reject)
{
    dispatch_async(dispatch_get_global_queue(DISPATCH_QUEUE_PRIORITY_DEFAULT, 0), ^{
        if (!self.model || !self.context) {
            reject(@"MODEL_NOT_LOADED", @"Model not loaded", nil);
            return;
        }

        NSString *prompt = config[@"prompt"];
        NSInteger maxTokens = [config[@"maxTokens"] integerValue];
        float temperature = [config[@"temperature"] floatValue];

        // í† í°í™”
        std::vector<llama_token> tokens;
        tokens.resize(llama_tokenize(self.model, [prompt UTF8String], strlen([prompt UTF8String]), tokens.data(), tokens.size(), true, false));

        // ë°°ì¹˜ ì„¤ì •
        llama_batch batch = llama_batch_init(tokens.size(), 0, 1);

        for (size_t i = 0; i < tokens.size(); i++) {
            batch.token[i] = tokens[i];
            batch.pos[i] = i;
            batch.seq_id[i] = 0;
            batch.logits[i] = i == tokens.size() - 1;
        }
        batch.n_tokens = tokens.size();

        // ì¶”ë¡  ì‹¤í–‰
        std::string result;
        for (int i = 0; i < maxTokens; i++) {
            llama_decode(self.context, batch);

            // ë‹¤ìŒ í† í° ìƒ˜í”Œë§
            llama_token newToken = llama_sample_token_greedy(self.context, nullptr);

            if (newToken == llama_token_eos(self.model)) {
                break;
            }

            // ê²°ê³¼ì— ì¶”ê°€
            char piece[256];
            int n = llama_token_to_piece(self.model, newToken, piece, sizeof(piece), true);
            result.append(piece, n);

            // ë°°ì¹˜ ì—…ë°ì´íŠ¸
            batch = llama_batch_init(1, 0, 1);
            batch.token[0] = newToken;
            batch.pos[0] = tokens.size() + i;
            batch.seq_id[0] = 0;
            batch.logits[0] = true;
            batch.n_tokens = 1;
        }

        llama_batch_free(batch);

        resolve(@(result.c_str()));
    });
}

RCT_EXPORT_METHOD(unloadModel:(nonnull NSNumber *)modelId
                  resolver:(RCTPromiseResolveBlock)resolve
                  rejecter:(RCTPromiseRejectBlock)reject)
{
    if (self.context) {
        llama_free(self.context);
        self.context = NULL;
    }

    if (self.model) {
        llama_free_model(self.model);
        self.model = NULL;
    }

    resolve(@{@"success": @YES});
}

@end
```

#### 7.3 í†µí•© í…ŒìŠ¤íŠ¸
```typescript
// __tests__/SLLMIntegration.test.tsx
import { renderHook, act } from '@testing-library/react-hooks';
import { useOnDeviceSLLM } from '../hooks/useOnDeviceSLLM';

describe('SLLM Integration Tests', () => {
  const mockConfig = {
    modelPath: 'models/qwen2-1.5b-big5.Q4_K_M.gguf',
    contextLength: 2048,
    temperature: 0.7
  };

  it('should load model within 5 seconds', async () => {
    const { result } = renderHook(() => useOnDeviceSLLM(mockConfig));

    const startTime = Date.now();

    await act(async () => {
      await result.current.loadModel();
    });

    const loadTime = Date.now() - startTime;

    expect(loadTime).toBeLessThan(5000);
    expect(result.current.isLoading).toBe(false);
    expect(result.current.error).toBe(null);
    expect(result.current.model).toBeTruthy();
  });

  it('should generate report within 15 seconds', async () => {
    const { result } = renderHook(() => useOnDeviceSLLM(mockConfig));

    // ë¨¼ì € ëª¨ë¸ ë¡œë“œ
    await act(async () => {
      await result.current.loadModel();
    });

    const testPrompt = buildBig5Prompt({
      openness: "ìƒˆë¡œìš´ ê²ƒì„ ë°°ìš°ëŠ” ê±¸ ì¢‹ì•„í•´ìš”",
      conscientiousness: "ê³„íšì„ ì„¸ìš°ê³  ì§€í‚¤ëŠ” ê²Œ ì¤‘ìš”í•´ìš”",
      extraversion: "í˜¼ì ìˆì„ ë•Œ ì—ë„ˆì§€ë¥¼ ì–»ì–´ìš”",
      agreeableness: "ì‚¬ëŒë“¤ì„ ë„ì™€ì£¼ëŠ” ê±¸ ì¦ê²¨ìš”",
      neuroticism: "ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ ë°›ìœ¼ë©´ ìš´ë™ìœ¼ë¡œ í’€ì–´ìš”"
    });

    const startTime = Date.now();

    await act(async () => {
      const response = await result.current.generateText(testPrompt);
    });

    const inferenceTime = Date.now() - startTime;

    expect(inferenceTime).toBeLessThan(15000);
    expect(result.current.isGenerating).toBe(false);
    expect(result.current.error).toBe(null);
  });

  it('should handle model loading errors gracefully', async () => {
    const invalidConfig = {
      ...mockConfig,
      modelPath: 'invalid/model/path.gguf'
    };

    const { result } = renderHook(() => useOnDeviceSLLM(invalidConfig));

    await act(async () => {
      await result.current.loadModel();
    });

    expect(result.current.isLoading).toBe(false);
    expect(result.current.error).toBeTruthy();
    expect(result.current.model).toBeFalsy();
  });
});
```

---

## ğŸ”§ ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### 1. í”í•œ ë¬¸ì œ ë° í•´ê²°ì±…

#### GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
```bash
# ë¬¸ì œ: CUDA out of memory
# í•´ê²°: ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
per_device_train_batch_size=1
gradient_accumulation_steps=16
```

#### ëª¨ë¸ í’ˆì§ˆ ì €í•˜
```python
# ë¬¸ì œ: íŠœë‹ í›„ ì„±ëŠ¥ ì €í•˜
# í•´ê²°: í•™ìŠµë¥  ì¡°ì •
learning_rate=5e-5  # ë” ë‚®ì€ í•™ìŠµë¥ 
num_train_epochs=5   # ë” ë§ì€ ì—í¬í¬
```

#### ì–‘ìí™” í›„ ì„±ëŠ¥ ì €í•˜
```bash
# ë¬¸ì œ: 4ë¹„íŠ¸ ì–‘ìí™” í›„ í’ˆì§ˆ ì €í•˜
# í•´ê²°: ë” ë†’ì€ ì–‘ìí™” ì‚¬ìš©
Q5_K_M  # 5ë¹„íŠ¸ K-Quants ì¤‘ê°„
Q8_0    # 8ë¹„íŠ¸ ì–‘ìí™”
```

### 2. ìµœì í™” íŒ

#### ë°ì´í„° í’ˆì§ˆ ìµœì í™”
- í˜ë¥´ì†Œë‚˜ ì¼ê´€ì„± ê²€ì¦ ìë™í™”
- ë°ì´í„° ì¦ê°• ê¸°ë²• ì ìš©
- ê³¨ë“œ ìŠ¤íƒ ë‹¤ë“œ ë°ì´í„° ë¹„ìœ¨ ë†’ì´ê¸°

#### íŠ¸ë ˆì´ë‹ ìµœì í™”
- ì¡°ê¸° ì¢…ë£Œ(early stopping) ì„¤ì •
- í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ íŠœë‹
- ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ì ìš©

#### ì¶”ë¡  ìµœì í™”
- ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ìµœì í™”
- ë°°ì¹˜ ì¶”ë¡  ê³ ë ¤
- ìºì‹± ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„

---

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### ëª©í‘œ ì„±ëŠ¥æŒ‡æ ‡
- **ëª¨ë¸ ë¡œë”©**: < 5ì´ˆ (iPhone 15 Pro)
- **ì¶”ë¡  ì†ë„**: < 15ì´ˆ (500í† í° ìƒì„±)
- **ì•± í¬ê¸°**: < 1.5GB (ëª¨ë¸ í¬í•¨)
- **ë©”ëª¨ë¦¬ ì‚¬ìš©**: < 2GB (í‰ê· )

### ëª¨ë‹ˆí„°ë§ ë„êµ¬
```python
# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
import psutil
import GPUtil

def monitor_system_resources():
    """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§"""

    # CPU ì‚¬ìš©ëŸ‰
    cpu_percent = psutil.cpu_percent(interval=1)

    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    memory = psutil.virtual_memory()

    # GPU ì‚¬ìš©ëŸ‰
    gpus = GPUtil.getGPUs()
    gpu_memory = gpus[0].memoryUtil * 100 if gpus else 0

    return {
        "cpu_percent": cpu_percent,
        "memory_percent": memory.percent,
        "gpu_memory_percent": gpu_memory
    }
```

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

ì´ ë¡œë“œë§µì„ í†µí•´ ì„±ê³µì ìœ¼ë¡œ ì˜¨ë””ë°”ì´ìŠ¤ LLMì„ ê°œë°œí•˜ì…¨ë‹¤ë©´:

1. **ì•± í†µí•©**: React Native ì•±ì— ì™„ì „íˆ í†µí•©
2. **ì„±ëŠ¥ ìµœì í™”**: ì‹¤ì œ ê¸°ê¸°ì—ì„œ í…ŒìŠ¤íŠ¸ ë° ìµœì í™”
3. **ì‚¬ìš©ì í…ŒìŠ¤íŠ¸**: Beta í…ŒìŠ¤í„°ë¥¼ í†µí•œ í’ˆì§ˆ ê²€ì¦
4. **ë°°í¬**: App Storeì— ì•± ì¶œì‹œ

## ğŸ“š ì¶”ê°€ í•™ìŠµ ìë£Œ

- [LLaMA-Factory ê³µì‹ ë¬¸ì„œ](https://github.com/hiyouga/LLaMA-Factory)
- [llama.cpp ê³µì‹ ë¬¸ì„œ](https://github.com/ggerganov/llama.cpp)
- [Qwen2 ëª¨ë¸ í—ˆë¸Œ](https://huggingface.co/Qwen/Qwen2-1.5B)
- [Expo ê³µì‹ ë¬¸ì„œ](https://docs.expo.dev/)

---

ì´ ë¡œë“œë§µì´ LLM ê°œë°œ ì—¬ì •ì— ë„ì›€ì´ ë˜ê¸°ë¥¼ ë°”ëë‹ˆë‹¤! ê¶ê¸ˆí•œ ì ì´ ìˆë‹¤ë©´ ì–¸ì œë“ ì§€ ì§ˆë¬¸í•´ì£¼ì„¸ìš”. ğŸš€