## 1. 개요

### 1.1. 문서 목적

본 문서는 '온디바이스 Big5 심리 분석 리포트 앱' 개발에 필요한 기능적, 비기능적 요구사항을 정의하는 것을 목적으로 한다. 본 문서는 기획자, 개발자, QA 간의 명확한 소통과 합의를 위한 기준으로 사용된다.

### 1.2. 프로젝트 목적

본 프로젝트의 최종 목표는 사용자의 Big5 성격 특성을 분석하는 심리학적 리포트를 **사용자 기기 내부에서(On-Device)** 직접 생성하여 제공하는 것이다. 이를 통해 다음 가치를 달성한다.

- **데이터 프라이버시 극대화:** 사용자의 민감한 답변(심리 상태)을 외부 서버로 전송하지 않는다.

- **오프라인 기능 제공:** 인터넷 연결 없이도 앱의 핵심 기능(분석 리포트 생성)을 사용할 수 있다.

- **즉각적인 경험:** 서버 지연 시간(latency) 없이 빠른 분석 결과를 제공한다.


### 1.3. 프로젝트 범위

#### 1.3.1. 포함 범위 (In-Scope)

- Big5 모델 기반 5개 질문 제공 및 답변 입력 UI

- 사용자 답변(텍스트)을 입력받아 분석 리포트(텍스트)를 생성하는 온디바이스 sLLM 탑재

- 생성된 심리 분석 리포트 표시 UI

- 모델 로딩 및 리포트 생성 중 상태 표시(로딩 인디케이터 등)


#### 1.3.2. 제외 범위 (Out-of-Scope)

- 사용자 계정 시스템 (회원가입, 로그인)

- 서버 기반 데이터 백업 및 동기화

- 리포트 외부 공유 기능 (단, 텍스트 복사 기능은 예외)

- Big5 외 다른 심리 검사 모델


---

## 2. 시스템 아키텍처

본 시스템은 Expo (React Native)를 클라이언트로 하며, `llama.cpp` 기반의 네이티브 모듈을 통해 온디바이스 sLLM(Qwen 기반)을 호출하는 구조를 가진다.

코드 스니펫

```
flowchart TD
    subgraph 📱 Expo App (React Native)
        A[UI: 질문 화면] -- 5개 답변 제출 --> B{JS: 프롬프트 생성기};
        B -- 생성된 프롬프트 --> C[Native Module Bridge (e.g., llama.rn)];
        C -- 결과(Report) 반환 --> D{JS: 상태 관리};
        D -- 리포트 텍스트 --> E[UI: 리포트 화면];
    end

    subgraph ⚙️ Native Layer (iOS)
        C <--> F[On-Device sLLM (llama.cpp)];
        F -- 구동 --> G[(GGUF 모델 파일)];
    end

    style F fill:#f9f,stroke:#333,stroke-width:2px
    style G fill:#lightgrey,stroke:#333,stroke-width:2px
```

---

## 3. 기능 요구사항

### 3.1. 질문 응답 (FR-01)

- **FR-01.1:** 사용자는 Big5 성격 특성(개방성, 성실성, 외향성, 우호성, 신경성)에 기반한 5개의 고정된 질문을 순차적으로 제공받는다.

- **FR-01.2:** 각 질문마다 사용자가 답변을 작성하는 데 참고할 수 있는 '힌트' 텍스트가 제공되어야 한다.

- **FR-01.3:** 각 질문의 답변은 자유 텍스트 형식이며, 최대 500자까지 입력할 수 있다.

- **FR-01.4:** 사용자는 현재 진행 상태(예: "3/5")를 시각적으로 인지할 수 있어야 한다.


### 3.2. 모델 구동 (FR-02)

- **FR-02.1:** 앱은 리포트 생성 요청 전에 온디바이스 sLLM을 메모리에 로드해야 한다.

- **FR-02.2:** 모델 로딩(초기화) 중에는 사용자에게 명확한 로딩 상태(예: "분석 엔진을 준비 중입니다...")를 표시해야 한다.

- **FR-02.3:** 모델 로드 실패 시, 사용자에게 오류 메시지와 함께 재시도 옵션을 제공해야 한다.


### 3.3. 리포트 생성 (FR-03)

- **FR-03.1:** 사용자가 5개의 답변을 모두 완료하고 '분석하기' 버튼을 누르면, 5개의 답변 텍스트가 조합되어 sLLM을 위한 단일 프롬프트로 변환되어야 한다.

- **FR-03.2:** 생성된 프롬프트는 즉시 온디바이스 sLLM에 전달되어 추론을 시작해야 한다.

- **FR-03.3:** 모델이 리포트를 생성하는 동안(추론 시간), 사용자에게 "리포트를 작성 중입니다..."와 같은 진행 상태를 표시해야 한다.

- **FR-03.4:** sLLM은 입력된 5개 답변을 기반으로 Big5 특성별 분석 내용과 종합 의견이 포함된 텍스트 리포트를 생성해야 한다.


### 3.4. 리포트 표시 (FR-04)

- **FR-04.1:** 생성된 텍스트 리포트는 사용자에게 가독성 높은 UI(적절한 폰트 크기, 줄 바꿈, 섹션 구분)로 제공되어야 한다.

- **FR-04.2:** 리포트 내용은 특성별(예: #개방성, #성실성...)로 명확히 구분되어야 한다.

- **FR-04.3:** 사용자는 생성된 리포트 텍스트 전체를 클립보드에 복사할 수 있어야 한다.


---

## 4. 비기능 요구사항

### 4.1. 성능 (NFR-01)

- **NFR-01.1 (타겟 디바이스):** Apple iPhone 15 Pro (A17 Pro 칩)를 1차 성능 벤치마크 기준으로 한다.

- **NFR-01.2 (모델 로딩):** 앱 실행 또는 기능 진입 시, 모델의 메모리 로드 시간은 5초 이내를 목표로 한다.

- **NFR-01.3 (추론 속도):** 5개의 답변(총 2,500자 이내) 입력 기준, 리포트 생성 완료까지의 총 추론 시간은 15초 이내를 목표로 한다.


### 4.2. 프라이버시 및 보안 (NFR-02)

- **NFR-02.1:** 사용자의 모든 답변 및 생성된 리포트 데이터는 사용자 기기 외부(서버 등)로 일절 전송되어서는 안 된다.

- **NFR-02.2:** 모든 데이터 처리는 기기 내부에서 완결되어야 한다(100% On-Device).


### 4.3. 리소스 (NFR-03)

- **NFR-03.1 (앱 용량):** 양자화된 sLLM (GGUF) 모델 파일을 포함한 앱의 총 설치 용량(iOS)은 1.5GB를 초과하지 않도록 권장한다.

- **NFR-03.2 (메모리):** 모델 로드 및 추론 시, 과도한 RAM 점유로 인해 OS에 의해 앱이 강제 종료(OOM, Out Of Memory)되지 않아야 한다.


### 4.4. 호환성 및 개발 환경 (NFR-04)

- **NFR-04.1:** 앱은 Expo (React Native) 프레임워크를 기반으로 개발한다.

- **NFR-04.2:** 네이티브 모듈(sLLM) 연동을 위해 **Expo Go**가 아닌 **Expo Development Client** 또는 **Prebuild** 방식을 사용해야 한다.

- **NFR-04.3:** 1차 출시 타겟은 iOS (최신 버전 - 2)으로 한다.


### 4.5. 사용성 (NFR-05)

- **NFR-05.1:** 앱의 모든 핵심 기능(질문 응답, 리포트 생성)은 인터넷 연결이 없는 오프라인 상태에서 완벽하게 동작해야 한다.


---

## 5. 제약 사항

- **C-01:** 리포트 생성 AI 모델은 Qwen 계열의 sLLM을 파인 튜닝하여 사용한다.

- **C-02:** 온디바이스 추론 엔진은 `llama.cpp` 및 GGUF 형식을 우선적으로 고려한다.

- **C-03:** 리포트의 품질은 100% 파인 튜닝된 sLLM의 성능에 의존하며, 정해진 규칙 기반(Rule-based) 로직을 사용하지 않는다.


---

## 6. 용어 정의

- **Big5:** 5대 성격 특성 모델. 개방성(Openness), 성실성(Conscientiousness), 외향성(Extraversion), 우호성(Agreeableness), 신경성(Neuroticism).

- **sLLM (Small Language Model):** 모바일 기기 등 제한된 리소스 환경에서 실행 가능하도록 파라미터 수를 줄인 경량화 언어 모델.

- **On-Device AI:** 외부 서버와의 통신 없이, 사용자 기기 내부의 연산 장치(NPU, GPU 등)를 사용하여 AI 모델을 실행하는 기술.

- **GGUF:** `llama.cpp` 라이브러리에서 사용하는 표준 모델 파일 형식. 양자화 및 모델 메타데이터를 포함한다.

- **Expo Dev Client:** Expo Go와 달리 커스텀 네이티브 코드를 포함할 수 있게 해주는 Expo 개발용 클라이언트 빌드.